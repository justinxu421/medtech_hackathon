{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from wrapper import XRAY\n",
    "from split import split_data\n",
    "\n",
    "image_folder = 'images'\n",
    "label_path = 'Data_Entry_2017.csv'\n",
    "stats_filepath = 'outputs_2.txt'\n",
    "n_classes = 1 # regression problem\n",
    "use_parallel = True\n",
    "vision_model = torchvision.models.inception_v3()\n",
    "\n",
    "loss_weights = torch.tensor([1.])\n",
    "if torch.cuda.is_available():\n",
    "    loss_weights = loss_weights.cuda()\n",
    "# squared error loss\n",
    "criterion = nn.MSELoss(size_average = False)\n",
    "optimizer_type = torch.optim.Adam\n",
    "lr_scheduler_type = optim.lr_scheduler.StepLR\n",
    "num_epochs = 10\n",
    "best_model_filepath = None\n",
    "best_model_filepath = 'different_loss_best_model.tar'\n",
    "load_model_filepath = 'different_loss_best_model.tar'\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Since imagenet has 1000 classes, we need to change our last layer to 1 so that we get a regression problem\n",
    "n_features = vision_model.fc.in_features\n",
    "vision_model.fc = nn.Linear(n_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, datasets, dataset_sizes, criterion, optimizer, scheduler, use_gpu, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    lowest_loss = 1000\n",
    "    \n",
    "    # list of models from all epochs\n",
    "    model_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                    model = model.cuda()\n",
    "                else:\n",
    "                    inputs = Variable(inputs)\n",
    "                    labels = Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                if type(outputs) == tuple:\n",
    "                    outputs, _ = outputs\n",
    "                loss = criterion(outputs, labels.reshape(-1,1).float())\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            with open(stats_filepath, 'a') as f:\n",
    "                f.write('Epoch {} {} Loss: {:.4f} \\n'.format(epoch, phase, epoch_loss))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':               \n",
    "                    \n",
    "                # update best model based on f1_score\n",
    "                if epoch_loss < lowest_loss:\n",
    "                    lowest_loss = epoch_loss\n",
    "                    best_model_wts = model.state_dict()\n",
    "\n",
    "                    state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "                    if best_model_filepath is not None:\n",
    "                        torch.save(state, best_model_filepath)\n",
    "        \n",
    "        model_list.append(copy.deepcopy(model))\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(lowest_loss))\n",
    "    with open(stats_filepath, 'a') as f:\n",
    "        f.write('Best val loss: {:4f}\\n'.format(lowest_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model_list, model\n",
    "\n",
    "# return the scores\n",
    "def evaluate_model(model, testset_loader, test_size, use_gpu):\n",
    "    model.train(False)  # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    scores = []\n",
    "    # Iterate over data\n",
    "    for inputs, labels in tqdm(testset_loader):\n",
    "        # TODO: wrap them in Variable?\n",
    "        if use_gpu:\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "            model = model.cuda()\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        if type(outputs) == tuple:\n",
    "            outputs, _ = outputs\n",
    "        scores.extend(outputs.data.tolist())\n",
    "        loss = criterion(outputs, labels.reshape(-1,1).float())\n",
    "        running_loss += loss.data[0]\n",
    "    average_loss = running_loss / test_size\n",
    "    return (average_loss, scores)\n",
    "\n",
    "def load_saved_model(filepath, model, optimizer=None):\n",
    "    state = torch.load(filepath)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    # Only need to load optimizer if you are going to resume training on the model\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('00000001_000.png', 2.0), ('00000001_001.png', 1.0), ('00000001_002.png', 2.0), ('00000002_000.png', 0.0), ('00000003_000.png', 0.0)]\n",
      "num experiments is 4999\n",
      "train filenames size:  3499\n",
      "validation filenames size:  750\n",
      "test filenames size:  750\n",
      "torch.Size([3, 299, 299])\n",
      "training dataset size:  3499\n",
      "validation dataset size:  750\n",
      "test dataset size:  750\n",
      "[Using all the available GPUs]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_filenames, val_filenames, test_filenames = split_data(label_path)\n",
    "print('train filenames size: ', len(train_filenames))\n",
    "print('validation filenames size: ', len(val_filenames))\n",
    "print('test filenames size: ', len(test_filenames))\n",
    "\n",
    "train_dataset = XRAY(image_folder, train_filenames)\n",
    "val_dataset = XRAY(image_folder, val_filenames)\n",
    "test_dataset = XRAY(image_folder, test_filenames)\n",
    "# print([y for img, y in train_dataset])\n",
    "# print([y for img, y in val_dataset])\n",
    "# print([y for img, y in test_dataset])\n",
    "\n",
    "#print out a sample image shape\n",
    "image_array, label = train_dataset[4]\n",
    "print(image_array.shape)\n",
    "print('training dataset size: ', len(train_dataset))\n",
    "print('validation dataset size: ', len(val_dataset))\n",
    "print('test dataset size: ', len(test_dataset))\n",
    "\n",
    "trainset_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=8)\n",
    "valset_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=8)\n",
    "testset_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=8)\n",
    "\n",
    "# To view which layers are freezed and which layers are not freezed:\n",
    "# for name, child in vision_model.named_children():\n",
    "#     for name_2, params in child.named_parameters():\n",
    "#         print(name_2, params.requires_grad)\n",
    "\n",
    "if use_parallel:\n",
    "    print(\"[Using all the available GPUs]\")\n",
    "    vision_model = nn.DataParallel(vision_model, device_ids=[0, 1])\n",
    "\n",
    "dataloaders = {'train': trainset_loader, 'val': valset_loader}\n",
    "datasets = {'train': train_dataset, 'val': val_dataset}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "optimizable_params = [param for param in vision_model.parameters() if param.requires_grad]\n",
    "optimizer = optimizer_type(optimizable_params, lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler_type(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# If we want to load a model with saved parameters\n",
    "# if load_model_filepath is not None:\n",
    "#     load_saved_model(load_model_filepath, vision_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5289767f4c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                              \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                              \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                              num_epochs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-18aa4ef4b644>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, datasets, dataset_sizes, criterion, optimizer, scheduler, use_gpu, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "model_list, best_model = train_model(vision_model,\n",
    "                             dataloaders,\n",
    "                             datasets,\n",
    "                             dataset_sizes,\n",
    "                             criterion,\n",
    "                             optimizer,\n",
    "                             exp_lr_scheduler,\n",
    "                             use_cuda,\n",
    "                             num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model_filepath is not None:\n",
    "    load_saved_model(load_model_filepath, vision_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "100%|██████████| 150/150 [00:11<00:00, 12.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1554, device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, scores = evaluate_model(vision_model, testset_loader, len(test_dataset), use_cuda)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "100%|██████████| 150/150 [00:11<00:00, 12.89it/s]\n"
     ]
    }
   ],
   "source": [
    "loss, scores = evaluate_model(vision_model, valset_loader, len(val_dataset), use_cuda)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts = np.array([x[1] for x in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9326733350753784, 0.5),\n",
       " (0.28071942925453186, 0.0),\n",
       " (0.3060198426246643, 0.0),\n",
       " (1.4439618587493896, 0.0),\n",
       " (0.4380379617214203, 0.0),\n",
       " (0.9185542464256287, 1.0),\n",
       " (1.0238196849822998, 0.0),\n",
       " (0.7362310886383057, 1.0),\n",
       " (1.280749797821045, 2.0),\n",
       " (0.36011141538619995, 0.0),\n",
       " (0.8754100203514099, 0.0),\n",
       " (0.9769331216812134, 2.0),\n",
       " (0.9596750140190125, 0.0),\n",
       " (1.0753707885742188, 0.0),\n",
       " (0.9337424039840698, 2.0),\n",
       " (1.3871585130691528, 0.0),\n",
       " (0.35273706912994385, 0.0),\n",
       " (0.7878090739250183, 0.0),\n",
       " (1.3945603370666504, 2.0),\n",
       " (1.136681079864502, 2.0),\n",
       " (0.8359692096710205, 1.0),\n",
       " (1.3284865617752075, 0.0),\n",
       " (0.8580381274223328, 2.0),\n",
       " (1.4534716606140137, 2.0),\n",
       " (0.5866388082504272, 0.0),\n",
       " (1.0372251272201538, 2.5),\n",
       " (0.7110192775726318, 2.0),\n",
       " (1.1165249347686768, 0.0),\n",
       " (1.1776072978973389, 0.0),\n",
       " (0.3762892782688141, 0.0),\n",
       " (0.39345210790634155, 3.0),\n",
       " (0.4482289254665375, 1.0),\n",
       " (0.3861467242240906, 0.0),\n",
       " (0.4071401059627533, 0.0),\n",
       " (0.39822468161582947, 0.0),\n",
       " (1.0495119094848633, 2.0),\n",
       " (0.704064130783081, 1.0),\n",
       " (1.4220982789993286, 2.0),\n",
       " (0.7239071130752563, 3.0),\n",
       " (0.3362013101577759, 0.0),\n",
       " (0.6062119007110596, 0.0),\n",
       " (0.709100604057312, 0.0),\n",
       " (0.48202452063560486, 0.0),\n",
       " (0.3813917338848114, 0.0),\n",
       " (0.1295211911201477, 0.0),\n",
       " (1.240465760231018, 2.0),\n",
       " (0.6104728579521179, 0.0),\n",
       " (0.8228788375854492, 0.0),\n",
       " (0.5428827404975891, 0.0),\n",
       " (0.37126779556274414, 2.0),\n",
       " (0.5014890432357788, 0.0),\n",
       " (0.3358224332332611, 0.0),\n",
       " (0.43888407945632935, 0.0),\n",
       " (0.9660851955413818, 0.0),\n",
       " (0.7089691162109375, 2.0),\n",
       " (0.8127884864807129, 0.0),\n",
       " (0.5366732478141785, 0.0),\n",
       " (1.093063235282898, 2.5),\n",
       " (0.3813987076282501, 0.0),\n",
       " (0.6421334147453308, 0.0),\n",
       " (0.23443076014518738, 0.0),\n",
       " (0.522135317325592, 0.0),\n",
       " (1.306641697883606, 1.3333333333333333),\n",
       " (0.3483191132545471, 0.0),\n",
       " (0.47669416666030884, 0.0),\n",
       " (1.0976167917251587, 0.0),\n",
       " (0.8292067050933838, 1.5),\n",
       " (0.6500152945518494, 2.5),\n",
       " (0.37427380681037903, 0.0),\n",
       " (0.8815617561340332, 0.0),\n",
       " (0.42634493112564087, 0.0),\n",
       " (0.4244266152381897, 0.0),\n",
       " (0.08622319996356964, 0.0),\n",
       " (1.443350076675415, 2.0),\n",
       " (1.5805208683013916, 0.0),\n",
       " (0.6556035280227661, 0.0),\n",
       " (1.3415648937225342, 0.0),\n",
       " (0.3447687327861786, 0.0),\n",
       " (0.4956675171852112, 0.0),\n",
       " (0.36256852746009827, 0.0),\n",
       " (0.7264609336853027, 0.0),\n",
       " (0.827276349067688, 0.0),\n",
       " (0.2850281894207001, 0.0),\n",
       " (0.7763862609863281, 2.0),\n",
       " (0.3289646506309509, 0.0),\n",
       " (0.6235501170158386, 0.0),\n",
       " (0.6206585168838501, 1.3333333333333333),\n",
       " (1.155600666999817, 2.0),\n",
       " (0.15083102881908417, 0.0),\n",
       " (0.33863919973373413, 0.0),\n",
       " (0.47032430768013, 0.0),\n",
       " (0.7999443411827087, 2.0),\n",
       " (1.0161796808242798, 1.6666666666666667),\n",
       " (0.8420426845550537, 0.0),\n",
       " (0.5902965664863586, 3.0),\n",
       " (0.849155843257904, 2.0),\n",
       " (0.33204758167266846, 0.0),\n",
       " (0.14720767736434937, 0.0),\n",
       " (0.6231445074081421, 0.0),\n",
       " (0.7641599774360657, 2.0),\n",
       " (1.0170553922653198, 0.0),\n",
       " (0.5558664798736572, 2.0),\n",
       " (1.3232769966125488, 2.0),\n",
       " (0.7007544636726379, 0.0),\n",
       " (1.2210825681686401, 2.5),\n",
       " (0.3196394741535187, 0.0),\n",
       " (1.450961709022522, 0.0),\n",
       " (0.8000354766845703, 0.0),\n",
       " (0.4617328643798828, 0.0),\n",
       " (0.33392345905303955, 0.0),\n",
       " (1.4107706546783447, 2.0),\n",
       " (0.6996124386787415, 1.0),\n",
       " (0.28210094571113586, 0.0),\n",
       " (0.33431288599967957, 2.0),\n",
       " (0.6477091908454895, 2.0),\n",
       " (1.2614024877548218, 0.0),\n",
       " (0.3255707323551178, 0.0),\n",
       " (0.8925566673278809, 2.0),\n",
       " (0.798644483089447, 1.6666666666666667),\n",
       " (1.463883638381958, 1.0),\n",
       " (0.6725503206253052, 0.0),\n",
       " (0.33184531331062317, 0.0),\n",
       " (0.3651679456233978, 0.0),\n",
       " (0.6442146301269531, 2.0),\n",
       " (0.7640048861503601, 0.0),\n",
       " (0.4895429313182831, 0.0),\n",
       " (0.47232210636138916, 0.0),\n",
       " (1.247632384300232, 2.0),\n",
       " (0.05120239406824112, 0.0),\n",
       " (1.3587121963500977, 2.0),\n",
       " (0.9930757880210876, 0.0),\n",
       " (0.8452457785606384, 0.0),\n",
       " (0.05741503834724426, 0.0),\n",
       " (0.8460491895675659, 2.0),\n",
       " (0.8223473429679871, 0.3333333333333333),\n",
       " (0.946884036064148, 2.0),\n",
       " (1.2053982019424438, 1.6666666666666667),\n",
       " (0.3922863304615021, 0.0),\n",
       " (0.9781094193458557, 0.0),\n",
       " (0.5184369087219238, 0.0),\n",
       " (0.9705942869186401, 1.0),\n",
       " (0.7320495247840881, 0.0),\n",
       " (0.5929204821586609, 0.0),\n",
       " (1.2043914794921875, 3.0),\n",
       " (1.133032202720642, 2.0),\n",
       " (0.47848114371299744, 0.0),\n",
       " (0.23394809663295746, 0.0),\n",
       " (0.8281528949737549, 2.0),\n",
       " (1.219197154045105, 1.3333333333333333),\n",
       " (0.3928353488445282, 0.0),\n",
       " (1.5949722528457642, 1.5),\n",
       " (0.8840489387512207, 0.0),\n",
       " (0.38168272376060486, 0.0),\n",
       " (0.8611394762992859, 1.3333333333333333),\n",
       " (0.9216676354408264, 0.0),\n",
       " (0.42812713980674744, 0.0),\n",
       " (0.6293278932571411, 0.0),\n",
       " (0.23003563284873962, 0.0),\n",
       " (0.5460814237594604, 1.0),\n",
       " (0.39583975076675415, 0.0),\n",
       " (0.12454210221767426, 0.0),\n",
       " (0.869623064994812, 0.0),\n",
       " (0.311245322227478, 2.0),\n",
       " (1.5345380306243896, 3.0),\n",
       " (0.7681822180747986, 0.0),\n",
       " (0.16782109439373016, 0.0),\n",
       " (0.36478859186172485, 0.0),\n",
       " (0.4636749029159546, 0.0),\n",
       " (0.3045889139175415, 0.0),\n",
       " (1.2754714488983154, 1.0),\n",
       " (1.454784631729126, 2.5),\n",
       " (0.8124352097511292, 0.0),\n",
       " (0.6612502336502075, 0.0),\n",
       " (0.9021397233009338, 0.0),\n",
       " (0.524682879447937, 0.0),\n",
       " (0.7425684332847595, 0.0),\n",
       " (1.523313045501709, 2.0),\n",
       " (0.6048869490623474, 0.0),\n",
       " (1.0963075160980225, 0.75),\n",
       " (0.7973482608795166, 0.0),\n",
       " (0.6600006818771362, 0.0),\n",
       " (0.5411537885665894, 0.0),\n",
       " (0.1623569130897522, 2.0),\n",
       " (0.5558375120162964, 0.0),\n",
       " (1.3503304719924927, 2.0),\n",
       " (1.21418035030365, 2.5),\n",
       " (1.0905792713165283, 2.0),\n",
       " (0.32208576798439026, 0.0),\n",
       " (0.21592570841312408, 0.0),\n",
       " (0.8768159747123718, 0.0),\n",
       " (0.9946972131729126, 0.0),\n",
       " (0.8030191659927368, 0.0),\n",
       " (0.5714641213417053, 2.0),\n",
       " (0.34998077154159546, 0.0),\n",
       " (0.08012450486421585, 0.0),\n",
       " (0.39520537853240967, 0.5),\n",
       " (0.9204950332641602, 0.0),\n",
       " (0.3170925974845886, 0.0),\n",
       " (1.3044021129608154, 0.0),\n",
       " (1.1117817163467407, 2.0),\n",
       " (0.6330400705337524, 0.0),\n",
       " (0.259428471326828, 0.0),\n",
       " (0.7608450055122375, 0.0),\n",
       " (0.7051421403884888, 0.0),\n",
       " (0.9844497442245483, 1.3333333333333333),\n",
       " (0.40136250853538513, 0.0),\n",
       " (1.5356532335281372, 0.0),\n",
       " (1.0651365518569946, 2.5),\n",
       " (0.7428825497627258, 0.0),\n",
       " (0.8980168104171753, 0.0),\n",
       " (1.2437556982040405, 0.0),\n",
       " (0.48114943504333496, 0.0),\n",
       " (0.19817091524600983, 0.0),\n",
       " (0.7661945819854736, 0.0),\n",
       " (0.865885317325592, 0.0),\n",
       " (0.26933375000953674, 0.0),\n",
       " (0.6690776348114014, 0.0),\n",
       " (0.27166303992271423, 0.6666666666666666),\n",
       " (0.430420458316803, 1.0),\n",
       " (0.793432354927063, 0.0),\n",
       " (1.0817698240280151, 1.0),\n",
       " (0.6635966300964355, 0.0),\n",
       " (0.46599411964416504, 0.0),\n",
       " (1.2247527837753296, 1.3333333333333333),\n",
       " (0.9306079149246216, 3.0),\n",
       " (0.3742983341217041, 0.0),\n",
       " (0.28077107667922974, 1.5),\n",
       " (1.1604305505752563, 0.0),\n",
       " (0.45403164625167847, 0.0),\n",
       " (0.4123675227165222, 0.0),\n",
       " (0.49899059534072876, 2.0),\n",
       " (1.0803167819976807, 2.5),\n",
       " (0.461839497089386, 0.0),\n",
       " (0.43679898977279663, 2.0),\n",
       " (0.14328308403491974, 0.0),\n",
       " (1.6978561878204346, 3.0),\n",
       " (1.1385282278060913, 3.0),\n",
       " (0.34674790501594543, 0.0),\n",
       " (0.8653711080551147, 0.0),\n",
       " (0.5767829418182373, 3.0),\n",
       " (0.6779061555862427, 0.0),\n",
       " (0.3757472336292267, 0.0),\n",
       " (0.4553782641887665, 0.0),\n",
       " (1.4371052980422974, 2.5),\n",
       " (0.5029932856559753, 0.0),\n",
       " (1.277889609336853, 2.0),\n",
       " (0.8470582962036133, 2.0),\n",
       " (0.43766942620277405, 0.0),\n",
       " (1.0642001628875732, 0.0),\n",
       " (1.2245136499404907, 2.0),\n",
       " (0.6995496153831482, 0.0),\n",
       " (0.4533223509788513, 0.0),\n",
       " (0.19732774794101715, 0.0),\n",
       " (0.32210052013397217, 0.0),\n",
       " (1.2794957160949707, 0.0),\n",
       " (1.041257381439209, 0.0),\n",
       " (1.4610129594802856, 1.0),\n",
       " (1.390201449394226, 3.0),\n",
       " (0.712546169757843, 2.0),\n",
       " (0.39571046829223633, 0.0),\n",
       " (1.1384164094924927, 0.0),\n",
       " (0.9986117482185364, 2.0),\n",
       " (0.37662118673324585, 0.0),\n",
       " (0.40712687373161316, 0.0),\n",
       " (0.3779107630252838, 2.0),\n",
       " (0.8451442718505859, 2.0),\n",
       " (0.6103119254112244, 0.0),\n",
       " (0.45292776823043823, 0.0),\n",
       " (0.1937418431043625, 0.0),\n",
       " (1.0242705345153809, 0.0),\n",
       " (0.7551794648170471, 0.0),\n",
       " (0.7225022912025452, 0.0),\n",
       " (1.2273736000061035, 3.0),\n",
       " (0.1884898692369461, 0.0),\n",
       " (0.85698401927948, 0.0),\n",
       " (0.6965915560722351, 0.0),\n",
       " (0.13200533390045166, 0.0),\n",
       " (0.7575798630714417, 0.0),\n",
       " (0.816335916519165, 0.5),\n",
       " (1.3807185888290405, 3.0),\n",
       " (0.8704458475112915, 2.0),\n",
       " (1.3842357397079468, 2.0),\n",
       " (0.785650372505188, 0.0),\n",
       " (0.35978367924690247, 0.0),\n",
       " (0.8068363666534424, 0.0),\n",
       " (1.00848388671875, 3.0),\n",
       " (0.6692728400230408, 2.0),\n",
       " (0.44680413603782654, 0.0),\n",
       " (0.5052610039710999, 0.0),\n",
       " (1.1851478815078735, 1.0),\n",
       " (0.9145278930664062, 2.0),\n",
       " (0.7162729501724243, 2.0),\n",
       " (0.7579023838043213, 0.0),\n",
       " (0.16307255625724792, 0.0),\n",
       " (1.4650202989578247, 0.0),\n",
       " (0.7084932327270508, 0.0),\n",
       " (0.8024120330810547, 0.0),\n",
       " (1.0129806995391846, 0.0),\n",
       " (0.6129984855651855, 0.0),\n",
       " (0.8205685615539551, 0.0),\n",
       " (1.2059773206710815, 2.5),\n",
       " (0.9416674375534058, 3.0),\n",
       " (1.3609100580215454, 0.0),\n",
       " (1.0380425453186035, 0.0),\n",
       " (0.4649418890476227, 0.0),\n",
       " (0.5290175080299377, 0.0),\n",
       " (1.0312716960906982, 2.5),\n",
       " (0.6969066858291626, 0.0),\n",
       " (0.23081226646900177, 0.0),\n",
       " (1.105815052986145, 1.0),\n",
       " (0.6904058456420898, 0.0),\n",
       " (0.9462982416152954, 0.0),\n",
       " (1.0082815885543823, 0.0),\n",
       " (0.5149441957473755, 0.0),\n",
       " (0.3301694989204407, 0.0),\n",
       " (0.8436796069145203, 1.0),\n",
       " (0.6365483999252319, 0.0),\n",
       " (1.1900931596755981, 2.0),\n",
       " (0.9604794383049011, 2.0),\n",
       " (1.4634662866592407, 0.0),\n",
       " (0.42318758368492126, 0.0),\n",
       " (0.43539103865623474, 0.0),\n",
       " (0.6934828758239746, 0.0),\n",
       " (1.171722173690796, 2.0),\n",
       " (0.9204887747764587, 1.0),\n",
       " (1.2097376585006714, 0.0),\n",
       " (1.1275980472564697, 0.0),\n",
       " (0.8420593738555908, 0.0),\n",
       " (1.4086331129074097, 2.25),\n",
       " (0.9421231150627136, 2.0),\n",
       " (0.39464229345321655, 0.0),\n",
       " (0.6874162554740906, 0.0),\n",
       " (0.42484885454177856, 0.0),\n",
       " (1.0233763456344604, 0.0),\n",
       " (1.4502073526382446, 2.0),\n",
       " (1.0172091722488403, 0.0),\n",
       " (0.5742596387863159, 2.0),\n",
       " (0.36545440554618835, 0.0),\n",
       " (0.7771199345588684, 2.0),\n",
       " (0.598534107208252, 0.0),\n",
       " (0.7191340327262878, 0.0),\n",
       " (0.09184715151786804, 0.0),\n",
       " (1.3995872735977173, 0.0),\n",
       " (0.642374575138092, 1.0),\n",
       " (0.7636235356330872, 0.0),\n",
       " (0.4891645908355713, 0.0),\n",
       " (1.4425245523452759, 2.0),\n",
       " (0.2941879630088806, 0.0),\n",
       " (1.3442612886428833, 2.5),\n",
       " (0.16436083614826202, 0.0),\n",
       " (1.2068417072296143, 2.0),\n",
       " (1.1023671627044678, 0.0),\n",
       " (1.3342255353927612, 2.0),\n",
       " (1.1524343490600586, 0.0),\n",
       " (0.663474440574646, 0.6666666666666666),\n",
       " (0.8537141680717468, 2.5),\n",
       " (1.1835638284683228, 0.0),\n",
       " (0.18426991999149323, 0.0),\n",
       " (1.369451880455017, 2.0),\n",
       " (0.6431799530982971, 0.0),\n",
       " (0.6779731512069702, 0.0),\n",
       " (0.47651195526123047, 0.0),\n",
       " (1.32790207862854, 2.0),\n",
       " (0.3246194124221802, 1.0),\n",
       " (0.7385730147361755, 0.0),\n",
       " (0.3572791814804077, 1.0),\n",
       " (1.171647548675537, 3.0),\n",
       " (0.8414098620414734, 0.0),\n",
       " (0.3262375593185425, 0.0),\n",
       " (1.393229365348816, 3.0),\n",
       " (0.3727891147136688, 0.0),\n",
       " (0.33957234025001526, 0.0),\n",
       " (1.079922080039978, 0.0),\n",
       " (0.32173556089401245, 0.0),\n",
       " (0.8382613062858582, 2.0),\n",
       " (0.9979272484779358, 2.0),\n",
       " (0.6964266300201416, 1.0),\n",
       " (0.2396637350320816, 0.0),\n",
       " (0.2727091312408447, 0.0),\n",
       " (0.5021885633468628, 0.0),\n",
       " (1.5084114074707031, 3.0),\n",
       " (1.1021959781646729, 2.0),\n",
       " (0.47714266180992126, 0.0),\n",
       " (1.3318579196929932, 2.3333333333333335),\n",
       " (1.4974595308303833, 0.0),\n",
       " (0.7340252995491028, 1.0),\n",
       " (0.47842615842819214, 0.0),\n",
       " (0.21623152494430542, 0.0),\n",
       " (1.0324877500534058, 0.0),\n",
       " (0.3185799717903137, 2.0),\n",
       " (0.3686893880367279, 0.0),\n",
       " (0.2709801495075226, 0.0),\n",
       " (1.1912446022033691, 0.0),\n",
       " (1.1140214204788208, 0.0),\n",
       " (0.31779924035072327, 0.0),\n",
       " (0.8256706595420837, 0.0),\n",
       " (0.24163325130939484, 0.0),\n",
       " (0.7796735763549805, 0.0),\n",
       " (0.7719859480857849, 0.0),\n",
       " (0.5023057460784912, 0.0),\n",
       " (1.2065335512161255, 2.5),\n",
       " (0.8532218933105469, 0.0),\n",
       " (0.36664846539497375, 0.0),\n",
       " (0.9485945105552673, 0.0),\n",
       " (0.16825725138187408, 0.0),\n",
       " (1.4324392080307007, 0.0),\n",
       " (1.3870044946670532, 2.0),\n",
       " (1.3495508432388306, 2.0),\n",
       " (0.5089358687400818, 1.0),\n",
       " (1.270586609840393, 2.5),\n",
       " (1.1098254919052124, 0.0),\n",
       " (1.0012444257736206, 2.0),\n",
       " (0.9666317701339722, 1.2),\n",
       " (0.23245827853679657, 0.0),\n",
       " (0.3927980959415436, 0.6666666666666666),\n",
       " (0.3786858320236206, 0.0),\n",
       " (0.3018549084663391, 0.0),\n",
       " (1.1327245235443115, 2.0),\n",
       " (0.30987051129341125, 0.0),\n",
       " (0.8903643488883972, 0.0),\n",
       " (1.1559978723526, 0.0),\n",
       " (0.9378940463066101, 0.0),\n",
       " (1.5287179946899414, 2.0),\n",
       " (0.46637287735939026, 0.0),\n",
       " (0.6654438972473145, 1.0),\n",
       " (0.9859728217124939, 2.0),\n",
       " (0.6321887969970703, 0.0),\n",
       " (0.7321813702583313, 1.0),\n",
       " (0.9927994012832642, 1.5),\n",
       " (1.2633116245269775, 2.5),\n",
       " (1.3141460418701172, 0.0),\n",
       " (0.4153038561344147, 2.0),\n",
       " (0.2954210340976715, 1.0),\n",
       " (0.8966761231422424, 0.0),\n",
       " (1.3520272970199585, 1.0),\n",
       " (0.28176599740982056, 0.0),\n",
       " (0.7546045184135437, 0.0),\n",
       " (0.8767027258872986, 0.0),\n",
       " (0.6760948896408081, 0.0),\n",
       " (0.2570149600505829, 0.0),\n",
       " (0.898697018623352, 0.0),\n",
       " (0.9563351273536682, 0.0),\n",
       " (0.8756324052810669, 0.0),\n",
       " (0.4840303361415863, 2.0),\n",
       " (0.22185148298740387, 0.0),\n",
       " (1.0765671730041504, 2.0),\n",
       " (0.7777882814407349, 3.0),\n",
       " (0.8815312385559082, 0.0),\n",
       " (1.2086403369903564, 2.0),\n",
       " (0.455534428358078, 0.0),\n",
       " (0.6070073843002319, 0.0),\n",
       " (0.09348397701978683, 0.0),\n",
       " (0.28730326890945435, 0.0),\n",
       " (0.10054787993431091, 0.0),\n",
       " (1.0354822874069214, 2.0),\n",
       " (0.7497813105583191, 2.0),\n",
       " (0.6375534534454346, 0.0),\n",
       " (1.2168998718261719, 0.0),\n",
       " (0.46522268652915955, 0.0),\n",
       " (0.8137005567550659, 0.0),\n",
       " (1.2528698444366455, 2.0),\n",
       " (1.317685604095459, 0.0),\n",
       " (0.5578959584236145, 0.0),\n",
       " (0.33947041630744934, 1.3333333333333333),\n",
       " (0.4280405640602112, 2.0),\n",
       " (0.5617278218269348, 0.0),\n",
       " (1.1658687591552734, 3.0),\n",
       " (1.5416265726089478, 2.0),\n",
       " (0.8244226574897766, 0.0),\n",
       " (0.06509900093078613, 0.0),\n",
       " (0.5618804097175598, 0.0),\n",
       " (0.7075104713439941, 1.5),\n",
       " (0.2568662166595459, 0.0),\n",
       " (0.6715267896652222, 3.0),\n",
       " (1.3495845794677734, 2.3333333333333335),\n",
       " (0.821323573589325, 0.0),\n",
       " (0.7195972800254822, 0.0),\n",
       " (1.3250141143798828, 0.0),\n",
       " (0.6534104943275452, 0.0),\n",
       " (1.2980241775512695, 1.3333333333333333),\n",
       " (0.7867422103881836, 1.0),\n",
       " (1.5881175994873047, 2.0),\n",
       " (0.5717552304267883, 1.0),\n",
       " (1.395054578781128, 0.0),\n",
       " (0.7534364461898804, 0.0),\n",
       " (0.5135948657989502, 0.0),\n",
       " (1.5991830825805664, 2.0),\n",
       " (0.8088417649269104, 0.0),\n",
       " (0.7717292308807373, 0.0),\n",
       " (0.24650560319423676, 0.0),\n",
       " (0.3067612051963806, 0.0),\n",
       " (0.34274429082870483, 0.0),\n",
       " (0.29319101572036743, 0.0),\n",
       " (0.8347045183181763, 0.0),\n",
       " (1.3404414653778076, 0.0),\n",
       " (0.8585149645805359, 1.5),\n",
       " (0.651063859462738, 0.0),\n",
       " (1.3057972192764282, 0.0),\n",
       " (0.8001339435577393, 1.0),\n",
       " (0.23405635356903076, 1.5),\n",
       " (1.270504355430603, 0.0),\n",
       " (0.9296079874038696, 1.0),\n",
       " (1.1458213329315186, 1.5),\n",
       " (1.4807909727096558, 0.0),\n",
       " (0.28890177607536316, 0.0),\n",
       " (0.8712424635887146, 0.0),\n",
       " (0.7968755960464478, 0.0),\n",
       " (1.668217420578003, 2.3333333333333335),\n",
       " (0.25967031717300415, 0.5),\n",
       " (0.32237887382507324, 2.0),\n",
       " (1.0328036546707153, 0.0),\n",
       " (0.9997936487197876, 1.3333333333333333),\n",
       " (0.7902462482452393, 0.6666666666666666),\n",
       " (1.1497559547424316, 2.5),\n",
       " (0.018727248534560204, 0.0),\n",
       " (0.9303804636001587, 2.0),\n",
       " (0.275520384311676, 0.0),\n",
       " (1.0637729167938232, 2.0),\n",
       " (0.5976855754852295, 0.0),\n",
       " (1.0826194286346436, 0.0),\n",
       " (1.3743422031402588, 2.3333333333333335),\n",
       " (1.0579324960708618, 2.0),\n",
       " (0.5015454888343811, 0.0),\n",
       " (0.7744261026382446, 0.0),\n",
       " (0.356609582901001, 0.0),\n",
       " (0.4955003559589386, 1.0),\n",
       " (0.5673785209655762, 0.6666666666666666),\n",
       " (1.2775088548660278, 0.0),\n",
       " (1.3088079690933228, 0.0),\n",
       " (0.8315922021865845, 0.0),\n",
       " (0.7568072080612183, 0.0),\n",
       " (0.9392592310905457, 0.0),\n",
       " (1.2110850811004639, 0.0),\n",
       " (0.6527193188667297, 0.0),\n",
       " (0.27192920446395874, 0.0),\n",
       " (0.8460506200790405, 3.0),\n",
       " (0.8034808039665222, 0.0),\n",
       " (0.9221457242965698, 0.0),\n",
       " (1.2247966527938843, 2.0),\n",
       " (0.7236800193786621, 0.0),\n",
       " (0.702694833278656, 0.0),\n",
       " (0.6777453422546387, 0.0),\n",
       " (0.7979404926300049, 0.0),\n",
       " (1.4945430755615234, 2.3333333333333335),\n",
       " (0.7475277185440063, 0.0),\n",
       " (0.4351935386657715, 0.0),\n",
       " (0.8143242597579956, 0.0),\n",
       " (0.2770192623138428, 2.0),\n",
       " (0.39167094230651855, 0.0),\n",
       " (0.31801626086235046, 0.0),\n",
       " (0.838811993598938, 2.25),\n",
       " (1.3536815643310547, 2.0),\n",
       " (1.0440257787704468, 0.0),\n",
       " (0.9764324426651001, 3.0),\n",
       " (0.14951756596565247, 0.0),\n",
       " (0.3754359185695648, 0.0),\n",
       " (0.9458727240562439, 0.0),\n",
       " (0.3676702380180359, 0.0),\n",
       " (0.530353844165802, 0.0),\n",
       " (0.8592841625213623, 2.0),\n",
       " (0.9432456493377686, 0.0),\n",
       " (1.0622010231018066, 0.0),\n",
       " (1.0685325860977173, 1.6666666666666667),\n",
       " (0.7023593783378601, 0.0),\n",
       " (1.0864043235778809, 2.0),\n",
       " (1.461767554283142, 3.0),\n",
       " (0.18957477807998657, 0.0),\n",
       " (0.49133509397506714, 0.0),\n",
       " (0.6197834014892578, 2.0),\n",
       " (0.3232213258743286, 0.0),\n",
       " (1.4901295900344849, 0.0),\n",
       " (0.21473507583141327, 0.0),\n",
       " (0.5681843757629395, 1.0),\n",
       " (1.055662751197815, 0.0),\n",
       " (0.9453415274620056, 0.0),\n",
       " (0.6888564825057983, 2.5),\n",
       " (0.42761337757110596, 0.0),\n",
       " (0.4526180624961853, 0.0),\n",
       " (0.3846453130245209, 0.0),\n",
       " (0.5348755717277527, 0.0),\n",
       " (1.5624734163284302, 2.3333333333333335),\n",
       " (0.7021734118461609, 0.0),\n",
       " (0.7402040958404541, 2.0),\n",
       " (0.8186652660369873, 0.0),\n",
       " (0.8977620601654053, 0.0),\n",
       " (0.11689402163028717, 2.0),\n",
       " (0.5238634347915649, 0.0),\n",
       " (1.4868718385696411, 2.0),\n",
       " (1.0132451057434082, 0.0),\n",
       " (0.18328507244586945, 0.0),\n",
       " (1.1044059991836548, 3.0),\n",
       " (0.34668105840682983, 0.0),\n",
       " (0.827199399471283, 0.0),\n",
       " (0.36342939734458923, 0.0),\n",
       " (0.7792025804519653, 0.0),\n",
       " (0.2978333532810211, 0.0),\n",
       " (0.8991903066635132, 1.3333333333333333),\n",
       " (1.1563348770141602, 0.0),\n",
       " (0.7956562042236328, 0.0),\n",
       " (0.5596221685409546, 0.0),\n",
       " (0.39289534091949463, 1.5),\n",
       " (1.282318115234375, 0.0),\n",
       " (0.7589078545570374, 2.0),\n",
       " (0.8518636226654053, 2.0),\n",
       " (1.0344067811965942, 0.0),\n",
       " (1.357482671737671, 2.0),\n",
       " (1.4113913774490356, 2.3333333333333335),\n",
       " (1.144416332244873, 2.0),\n",
       " (0.4546930491924286, 2.3333333333333335),\n",
       " (0.7646876573562622, 0.0),\n",
       " (0.393229603767395, 0.0),\n",
       " (0.7399739623069763, 1.0),\n",
       " (0.7380518317222595, 0.0),\n",
       " (0.65283203125, 0.0),\n",
       " (0.5388069748878479, 0.0),\n",
       " (1.31931734085083, 1.0),\n",
       " (0.7423169016838074, 1.5),\n",
       " (0.4279451370239258, 0.0),\n",
       " (0.8503415584564209, 0.0),\n",
       " (0.41670531034469604, 0.0),\n",
       " (0.752656102180481, 0.0),\n",
       " (1.3643172979354858, 1.6666666666666667),\n",
       " (0.8264805674552917, 0.0),\n",
       " (0.07218526303768158, 0.0),\n",
       " (0.761688232421875, 2.0),\n",
       " (1.497763991355896, 0.0),\n",
       " (0.9201352596282959, 1.3333333333333333),\n",
       " (1.0284377336502075, 0.0),\n",
       " (1.0084768533706665, 2.0),\n",
       " (0.708525538444519, 0.0),\n",
       " (0.7598689794540405, 0.0),\n",
       " (0.8437669277191162, 2.0),\n",
       " (0.7732054591178894, 0.0),\n",
       " (0.31067466735839844, 0.0),\n",
       " (0.767097532749176, 0.0),\n",
       " (1.2977975606918335, 1.0),\n",
       " (0.26787614822387695, 2.0),\n",
       " (0.356932133436203, 0.0),\n",
       " (0.18690340220928192, 0.0),\n",
       " (0.28989720344543457, 0.0),\n",
       " (0.1784595102071762, 0.0),\n",
       " (1.479623794555664, 2.0),\n",
       " (1.447291374206543, 2.0),\n",
       " (1.2464559078216553, 0.0),\n",
       " (0.4374590218067169, 2.0),\n",
       " (0.2152971774339676, 0.0),\n",
       " (1.3052687644958496, 2.0),\n",
       " (1.134441614151001, 1.5),\n",
       " (0.38857585191726685, 0.0),\n",
       " (0.4291880428791046, 0.0),\n",
       " (1.050474762916565, 0.0),\n",
       " (0.3847096860408783, 2.0),\n",
       " (0.4940688908100128, 2.0),\n",
       " (1.5675874948501587, 2.0),\n",
       " (0.8107971549034119, 2.0),\n",
       " (0.21768563985824585, 0.0),\n",
       " (1.1895204782485962, 1.0),\n",
       " (0.6532994508743286, 0.0),\n",
       " (1.4510602951049805, 2.0),\n",
       " (0.7676622867584229, 0.0),\n",
       " (0.29699403047561646, 0.0),\n",
       " (1.4655661582946777, 2.0),\n",
       " (0.8854268789291382, 2.0),\n",
       " (1.279741644859314, 1.6666666666666667),\n",
       " (1.277799367904663, 2.5),\n",
       " (1.4515392780303955, 2.0),\n",
       " (0.6072917580604553, 0.0),\n",
       " (1.1519200801849365, 0.0),\n",
       " (0.8940874934196472, 0.0),\n",
       " (0.24527426064014435, 0.0),\n",
       " (0.8555618524551392, 0.0),\n",
       " (0.8026987910270691, 0.0),\n",
       " (1.406391978263855, 0.0),\n",
       " (0.9201199412345886, 1.0),\n",
       " (0.8373450636863708, 2.0),\n",
       " (0.3761225640773773, 2.0),\n",
       " (0.28918036818504333, 0.0),\n",
       " (0.6818432211875916, 1.5),\n",
       " (1.461125135421753, 0.0),\n",
       " (0.9302377104759216, 0.0),\n",
       " (0.7065512537956238, 0.0),\n",
       " (0.2663935422897339, 0.0),\n",
       " (1.1451964378356934, 0.0),\n",
       " (0.23072075843811035, 0.0),\n",
       " (0.20520292222499847, 0.0),\n",
       " (0.8357129096984863, 0.0),\n",
       " (0.28964245319366455, 0.0),\n",
       " (0.5454983115196228, 2.0),\n",
       " (0.8386015892028809, 0.0),\n",
       " (0.45326030254364014, 0.0),\n",
       " (1.3195995092391968, 2.0),\n",
       " (1.240931749343872, 2.0),\n",
       " (0.7054370641708374, 2.0),\n",
       " (0.7624072432518005, 0.0),\n",
       " (0.8546141982078552, 0.0),\n",
       " (0.45165833830833435, 0.0),\n",
       " (1.603387713432312, 0.0),\n",
       " (1.149398684501648, 2.0),\n",
       " (1.3389666080474854, 2.3333333333333335),\n",
       " (0.5516008138656616, 1.75),\n",
       " (0.19901683926582336, 0.0),\n",
       " (0.25577861070632935, 0.0),\n",
       " (0.38694074749946594, 2.0),\n",
       " (0.4242228865623474, 0.0),\n",
       " (1.2832257747650146, 0.0),\n",
       " (0.3446066677570343, 0.0),\n",
       " (0.9112842679023743, 2.0),\n",
       " (0.47755905985832214, 2.0),\n",
       " (0.25626271963119507, 0.0),\n",
       " (0.4298195540904999, 0.0),\n",
       " (0.7015577554702759, 0.0),\n",
       " (0.41049686074256897, 0.0),\n",
       " (1.103522777557373, 0.0),\n",
       " (0.48151248693466187, 0.0),\n",
       " (0.2202892005443573, 0.0),\n",
       " (0.3684159815311432, 0.0),\n",
       " (0.8396081924438477, 0.0),\n",
       " (0.16700400412082672, 0.0),\n",
       " (1.2214144468307495, 2.0),\n",
       " (0.7298951148986816, 0.0),\n",
       " (0.6025832891464233, 0.0),\n",
       " (1.4293065071105957, 0.0),\n",
       " (0.6541154980659485, 2.0),\n",
       " (0.06264938414096832, 0.0),\n",
       " (0.6668664813041687, 0.0),\n",
       " (0.8687150478363037, 0.0),\n",
       " (0.3265196681022644, 0.0),\n",
       " (0.930067777633667, 2.0),\n",
       " (0.2872941792011261, 0.0),\n",
       " (0.3837922513484955, 0.0),\n",
       " (0.4011220335960388, 0.0),\n",
       " (0.5894367098808289, 0.0),\n",
       " (0.32600054144859314, 0.0),\n",
       " (0.3869470953941345, 0.0),\n",
       " (0.434439092874527, 2.5),\n",
       " (0.44443202018737793, 0.0),\n",
       " (0.5109086036682129, 0.0),\n",
       " (0.9710419774055481, 0.0),\n",
       " (0.9258419275283813, 1.3333333333333333),\n",
       " (1.4013198614120483, 2.0),\n",
       " (0.32035893201828003, 0.0),\n",
       " (0.9940142631530762, 2.0),\n",
       " (0.5138162970542908, 0.0),\n",
       " (1.0817996263504028, 0.0),\n",
       " (0.7905282974243164, 0.0),\n",
       " (0.6645076274871826, 1.5),\n",
       " (0.04542521759867668, 0.0),\n",
       " (0.3754766285419464, 0.5),\n",
       " (0.6810306906700134, 2.0),\n",
       " (1.0222523212432861, 2.0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.array(scores).reshape(-1)\n",
    "stacked = [(i,j) for i,j in zip(scores, test_counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores = sorted(stacked, key = lambda x: x[0])[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 33,\n",
       " 1.0: 6,\n",
       " 1.3333333333333333: 2,\n",
       " 1.5: 1,\n",
       " 1.6666666666666667: 2,\n",
       " 2.0: 35,\n",
       " 2.25: 1,\n",
       " 2.3333333333333335: 8,\n",
       " 2.5: 5,\n",
       " 3.0: 7}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "top_scores_counts = [x[1] for x in top_scores if x[1] >= 2]\n",
    "print(len(top_scores_counts)/len(top_scores))\n",
    "df = pd.DataFrame([x[1] for x in top_scores])\n",
    "df.stack().value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = ([x[1] for x in test_dataset if x[1] >= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25466666666666665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2.0: 136, 2.25: 2, 2.3333333333333335: 9, 2.5: 20, 3.0: 24}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (len(counts) / len(test_dataset))\n",
    "df = pd.DataFrame([counts])\n",
    "df.stack().value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'state_dict': vision_model.state_dict()}\n",
    "torch.save(state, 'smaller_model.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
